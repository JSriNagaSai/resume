{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAYcuLTYaVRUoGD1+fMK2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSriNagaSai/resume/blob/main/resume_match.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wxta8SQY0RPV",
        "outputId": "1b09a9c3-77c9-43b9-c4f2-46f5d9f298fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JSriNagaSai/resume"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv8PtTF83kUI",
        "outputId": "a7cf32a0-719c-4ad2-dd2c-1aa2086ba6a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'resume'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 131 (delta 3), reused 25 (delta 3), pack-reused 106\u001b[K\n",
            "Receiving objects: 100% (131/131), 2.74 MiB | 20.48 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install docx\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4TAmLnO61cz",
        "outputId": "ce4346c2-6423-4e19-f166-6ebf39c8a3e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53895 sha256=e52ee089ba83321e7fc567eb642b3018f5644d561a807180143e01b3c2dbcd68\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f5/1d/e09ba2c1907a43a4146d1189ae4733ca1a3bfe27ee39507767\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tup_pT07AlH",
        "outputId": "2c90076b-5114-4a7b-d7fc-01a84f33177d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Document\n",
            "  Downloading document-1.0.zip (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Document\n",
            "  Building wheel for Document (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Document: filename=document-1.0-py3-none-any.whl size=8765 sha256=0f863d6df01b3e354800fc5157853d0d4e8fea1e56431d8e0586380483511faa\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/ac/8e/4bf7a4f454fcbd96af3be9f23268ee72151302643d5b8d5290\n",
            "Successfully built Document\n",
            "Installing collected packages: Document\n",
            "Successfully installed Document-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NPyDIfz8XLg",
        "outputId": "fcde62cb-ed2a-4856-d248-f081bf403647"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/239.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m194.6/239.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set up NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "def preprocess_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = ''\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            text += pdf_reader.pages[page_num].extract_text()\n",
        "    return preprocess_text(text)\n",
        "\n",
        "def preprocess_docx(docx_path):\n",
        "    doc = Document(docx_path)\n",
        "    text = ''\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text += paragraph.text + '\\n'\n",
        "    return preprocess_text(text)\n",
        "\n",
        "def preprocess_file(file_path):\n",
        "    _, extension = os.path.splitext(file_path)\n",
        "    if extension == '.pdf':\n",
        "        return preprocess_pdf(file_path)\n",
        "    elif extension == '.docx':\n",
        "        return preprocess_docx(file_path)\n",
        "    else:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        return preprocess_text(text)\n",
        "\n",
        "def preprocess_resume_directory(input_directory, output_directory):\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    for filename in os.listdir(input_directory):\n",
        "        input_file_path = os.path.join(input_directory, filename)\n",
        "        output_file_path = os.path.join(output_directory, filename)\n",
        "\n",
        "        preprocessed_text = preprocess_file(input_file_path)\n",
        "\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "            file.write(preprocessed_text)\n",
        "\n",
        "        print(f'Preprocessed: {filename}')\n",
        "\n",
        "    print('Preprocessing complete.')\n",
        "\n",
        "def main():\n",
        "    # Define input and output directories\n",
        "    input_directory = '/content/resume/resumes'\n",
        "    output_directory = '/content/resume/preprocessed_resumes'\n",
        "\n",
        "    # Preprocess resumes\n",
        "    preprocess_resume_directory(input_directory, output_directory)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c_tKPfQ6rrA",
        "outputId": "6f438b85-ff69-4f34-b5d7-fdb303855be7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed: 90867631.pdf\n",
            "Preprocessed: 68460556.pdf\n",
            "Preprocessed: 18301617.pdf\n",
            "Preprocessed: 91121135.pdf\n",
            "Preprocessed: 38753827.pdf\n",
            "Preprocessed: 27372171.pdf\n",
            "Preprocessed: 51639418.pdf\n",
            "Preprocessed: 14789139.pdf\n",
            "Preprocessed: 91697974.pdf\n",
            "Preprocessed: 16186411.pdf\n",
            "Preprocessed: 26480367.pdf\n",
            "Preprocessed: 20001721.pdf\n",
            "Preprocessed: 25990239.pdf\n",
            "Preprocessed: 25905275.pdf\n",
            "Preprocessed: 28672970.pdf\n",
            "Preprocessed: 28697203.pdf\n",
            "Preprocessed: 20237244.pdf\n",
            "Preprocessed: 24020470.pdf\n",
            "Preprocessed: 12635195.pdf\n",
            "Preprocessed: 10265057.pdf\n",
            "Preprocessed: 29075857.pdf\n",
            "Preprocessed: 13836471.pdf\n",
            "Preprocessed: 64017585.pdf\n",
            "Preprocessed: 39413067.pdf\n",
            "Preprocessed: 41344156.pdf\n",
            "Preprocessed: 15802627.pdf\n",
            "Preprocessed: 17641670.pdf\n",
            "Preprocessed: 24913648.pdf\n",
            "Preprocessed: 28035460.pdf\n",
            "Preprocessed: 25207620.pdf\n",
            "Preprocessed: 12763627.pdf\n",
            "Preprocessed: 23666211.pdf\n",
            "Preprocessed: 15118506.pdf\n",
            "Preprocessed: 10247517.pdf\n",
            "Preprocessed: 66832845.pdf\n",
            "Preprocessed: 20024870.pdf\n",
            "Preprocessed: 16533554.pdf\n",
            "Preprocessed: 36434348.pdf\n",
            "Preprocessed: 19201175.pdf\n",
            "Preprocessed: 31111279.pdf\n",
            "Preprocessed: 18752129.pdf\n",
            "Preprocessed: 79541391.pdf\n",
            "Preprocessed: 15297298.pdf\n",
            "Preprocessed: 11584809.pdf\n",
            "Preprocessed: 19850482.pdf\n",
            "Preprocessed: 11580408.pdf\n",
            "Preprocessed: 92069209.pdf\n",
            "Preprocessed: 28897981.pdf\n",
            "Preprocessed: 40018190.pdf\n",
            "Preprocessed: 10553553.pdf\n",
            "Preprocessed: 57002858.pdf\n",
            "Preprocessed: 35325329.pdf\n",
            "Preprocessed: 52246737.pdf\n",
            "Preprocessed: 26768723.pdf\n",
            "Preprocessed: 17987433.pdf\n",
            "Preprocessed: 24083609.pdf\n",
            "Preprocessed: 28126340.pdf\n",
            "Preprocessed: 20879311.pdf\n",
            "Preprocessed: 13405733.pdf\n",
            "Preprocessed: 26801767.pdf\n",
            "Preprocessed: 24889109.pdf\n",
            "Preprocessed: 11957080.pdf\n",
            "Preprocessed: 16899268.pdf\n",
            "Preprocessed: 24230851.pdf\n",
            "Preprocessed: 18176523.pdf\n",
            "Preprocessed: 12045067.pdf\n",
            "Preprocessed: 25959103.pdf\n",
            "Preprocessed: 27058381.pdf\n",
            "Preprocessed: 51363762.pdf\n",
            "Preprocessed: 33241454.pdf\n",
            "Preprocessed: 21283365.pdf\n",
            "Preprocessed: 31243710.pdf\n",
            "Preprocessed: 25857360.pdf\n",
            "Preprocessed: 20408458.pdf\n",
            "Preprocessed: 23527321.pdf\n",
            "Preprocessed: 13477922.pdf\n",
            "Preprocessed: 27295996.pdf\n",
            "Preprocessed: 10839851.pdf\n",
            "Preprocessed: 36856210.pdf\n",
            "Preprocessed: 39718499.pdf\n",
            "Preprocessed: 17688766.pdf\n",
            "Preprocessed: 21780877.pdf\n",
            "Preprocessed: 18187364.pdf\n",
            "Preprocessed: 52618188.pdf\n",
            "Preprocessed: 22450718.pdf\n",
            "Preprocessed: 23864648.pdf\n",
            "Preprocessed: 91635250.pdf\n",
            "Preprocessed: 20674668.pdf\n",
            "Preprocessed: 27770859.pdf\n",
            "Preprocessed: 19796840.pdf\n",
            "Preprocessed: 70089206.pdf\n",
            "Preprocessed: 83816738.pdf\n",
            "Preprocessed: 20824105.pdf\n",
            "Preprocessed: 29975124.pdf\n",
            "Preprocessed: 15791766.pdf\n",
            "Preprocessed: 17681064.pdf\n",
            "Preprocessed: 33381211.pdf\n",
            "Preprocessed: 17111768.pdf\n",
            "Preprocessed: 13385306.pdf\n",
            "Preprocessed: 37764298.pdf\n",
            "Preprocessed: 22776912.pdf\n",
            "Preprocessed: 18159866.pdf\n",
            "Preprocessed: 12334140.pdf\n",
            "Preprocessed: 46260230.pdf\n",
            "Preprocessed: 26746496.pdf\n",
            "Preprocessed: 10840430.pdf\n",
            "Preprocessed: 30223363.pdf\n",
            "Preprocessed: 37242217.pdf\n",
            "Preprocessed: 81761658.pdf\n",
            "Preprocessed: 24038620.pdf\n",
            "Preprocessed: 15651486.pdf\n",
            "Preprocessed: 48037995.pdf\n",
            "Preprocessed: 10641230.pdf\n",
            "Preprocessed: 32959732.pdf\n",
            "Preprocessed: 89413122.pdf\n",
            "Preprocessed: 18067556.pdf\n",
            "Preprocessed: 29051656.pdf\n",
            "Preprocessed: 27485716.pdf\n",
            "Preprocessed: 27536013.pdf\n",
            "Preprocessing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text and remove stopwords and punctuation\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return tokens\n",
        "\n",
        "def generate_embeddings(job_descriptions):\n",
        "    embeddings = []\n",
        "    for job_description in job_descriptions:\n",
        "        # Preprocess the job description text\n",
        "        tokens = preprocess_text(job_description)\n",
        "        # Convert tokens to string and append to embeddings list\n",
        "        embeddings.append(\" \".join(tokens))\n",
        "    return embeddings\n",
        "\n",
        "def save_embeddings_to_file(embeddings, output_file):\n",
        "    with open(output_file, \"w\") as file:\n",
        "        for embedding in embeddings:\n",
        "            file.write(embedding + \"\\n\")\n",
        "\n",
        "def main():\n",
        "    # Sample job descriptions\n",
        "    job_descriptions = [\n",
        "        \"We are seeking a DevOps engineer to automate and streamline our organization's software development and deployment processes. The candidate should have experience with continuous integration/continuous deployment (CI/CD) tools and cloud platforms like AWS or Azure. Responsibilities include building and maintaining CI/CD pipelines, provisioning infrastructure, and monitoring system performance.\"\n",
        "    ]\n",
        "\n",
        "    # Generate embeddings for job descriptions\n",
        "    job_embeddings = generate_embeddings(job_descriptions)\n",
        "\n",
        "    # Save the embeddings to a text file\n",
        "    output_file = \"/content/resume/job_description_embeddings.txt\"\n",
        "    save_embeddings_to_file(job_embeddings, output_file)\n",
        "    print(f\"Embeddings saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlAY-zCq8hNC",
        "outputId": "da0fcf7b-5038-4b59-bc36-64796e05d0b2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings saved to /content/resume/job_description_embeddings.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy**"
      ],
      "metadata": {
        "id": "z4D3WLO3_s5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "def generate_embeddings(input_directory, output_directory):\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # Load English model with pre-trained word vectors\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Read preprocessed resume files\n",
        "    for filename in os.listdir(input_directory):\n",
        "        input_file_path = os.path.join(input_directory, filename)\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            doc = nlp(text)\n",
        "\n",
        "            # Extract word vectors and save to output file\n",
        "            output_file_path = os.path.join(output_directory, filename.replace('.txt', '.vec'))\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as out_file:\n",
        "                for token in doc:\n",
        "                    if token.has_vector:\n",
        "                        embedding = token.vector\n",
        "                        embedding_str = ' '.join(str(value) for value in embedding)\n",
        "                        out_file.write(f'{embedding_str}\\n')\n",
        "\n",
        "    print('Embedding generation complete.')\n",
        "\n",
        "def main():\n",
        "    # Define input and output directories\n",
        "    input_directory = '/content/resume/preprocessed_resumes'\n",
        "    output_directory = '/content/resume/resume_embeddings_spacy'\n",
        "\n",
        "    # Generate embeddings for preprocessed resumes\n",
        "    generate_embeddings(input_directory, output_directory)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESCGSf2e_fwi",
        "outputId": "7ceb1176-b060-4a0e-d078-dd1ae9433ede"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding generation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy and cosine**"
      ],
      "metadata": {
        "id": "mHoVZztQA43y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return text\n",
        "\n",
        "def generate_embeddings(input_directory):\n",
        "    embeddings = []\n",
        "    filenames = []\n",
        "\n",
        "    # Load the language model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Iterate over resumes in the input directory\n",
        "    for filename in os.listdir(input_directory):\n",
        "        input_file_path = os.path.join(input_directory, filename)\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            doc = nlp(preprocessed_text)\n",
        "            embeddings.append(doc.vector)\n",
        "            filenames.append((filename.split('.')[0], filename))  # Store candidate name along with filename\n",
        "\n",
        "    return np.array(embeddings), filenames\n",
        "\n",
        "def calculate_similarity(job_embedding, resume_embeddings):\n",
        "    return cosine_similarity(job_embedding.reshape(1, -1), resume_embeddings)[0]\n",
        "\n",
        "def match_job(job_description, resume_embeddings, filenames):\n",
        "    # Preprocess job description\n",
        "    preprocessed_job = preprocess_text(job_description)\n",
        "    # Generate job embedding\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    job_doc = nlp(preprocessed_job)\n",
        "    job_embedding = job_doc.vector\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_scores = calculate_similarity(job_embedding, resume_embeddings)\n",
        "\n",
        "    # Rank resumes based on similarity scores\n",
        "    ranked_resumes = sorted(zip(filenames, similarity_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_resumes\n",
        "\n",
        "def main():\n",
        "    # Define input directories\n",
        "    job_description_file = \"/content/resume/job_description_embeddings.txt\"\n",
        "    resumes_directory = \"/content/resume/preprocessed_resumes\"\n",
        "\n",
        "    # Generate embeddings for resumes\n",
        "    resume_embeddings, filenames = generate_embeddings(resumes_directory)\n",
        "\n",
        "    # Load job description\n",
        "    with open(job_description_file, 'r', encoding='utf-8') as file:\n",
        "        job_desc_text = file.read()\n",
        "\n",
        "    # Match job with resumes\n",
        "    matched_resumes = match_job(job_desc_text, resume_embeddings, filenames)\n",
        "\n",
        "    # Print the top matched resumes\n",
        "    print(\"Top Matched Resumes for the Following Job Description:\\n\")\n",
        "\n",
        "    # Print job description\n",
        "    print(\"Job Description:\\n----------------\")\n",
        "    print(job_desc_text + \"\\n\")\n",
        "\n",
        "    # Print matched resumes\n",
        "    print(\"Matched Resumes:\\n----------------\")\n",
        "    print(\"| Candidate Name | Resume File  | Similarity Score (%) |\")\n",
        "    print(\"|----------------|--------------|-----------------------|\")\n",
        "    for (candidate_name, resume_file), score in matched_resumes[:5]:  # Adjust the number of top matches to display\n",
        "        similarity_percentage = score * 100\n",
        "        print(f\"| {candidate_name:<15} | {resume_file:<12} | {similarity_percentage:.2f}                   |\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfNEQHglALTZ",
        "outputId": "52692aa4-874f-4134-9f5f-7c41bd7617e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Matched Resumes for the Following Job Description:\n",
            "\n",
            "Job Description:\n",
            "----------------\n",
            "seek DevOps engineer automate streamline organization software development deployment process candidate experience continuous integration continuous deployment CI CD tool cloud platform like aw Azure responsibility include build maintain CI cd pipeline provision infrastructure monitor system performance\n",
            "\n",
            "\n",
            "Matched Resumes:\n",
            "----------------\n",
            "| Candidate Name | Resume File  | Similarity Score (%) |\n",
            "|----------------|--------------|-----------------------|\n",
            "| 81761658        | 81761658.pdf | 95.67                   |\n",
            "| 27295996        | 27295996.pdf | 94.59                   |\n",
            "| 36434348        | 36434348.pdf | 94.49                   |\n",
            "| 26480367        | 26480367.pdf | 94.11                   |\n",
            "| 24230851        | 24230851.pdf | 94.07                   |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Jjn1qMxBEPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return text\n",
        "\n",
        "def generate_embeddings(input_directory):\n",
        "    embeddings = []\n",
        "    filenames = []\n",
        "\n",
        "    # Load the language model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Iterate over resumes in the input directory\n",
        "    for filename in os.listdir(input_directory):\n",
        "        input_file_path = os.path.join(input_directory, filename)\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            preprocessed_text = preprocess_text(text)\n",
        "            doc = nlp(preprocessed_text)\n",
        "            embeddings.append(doc.vector)\n",
        "            filenames.append((filename.split('.')[0], filename))  # Store candidate name along with filename\n",
        "\n",
        "    return np.array(embeddings), filenames\n",
        "\n",
        "def semantic_similarity_sbert_base_v2(job_embedding,resume_embeddings):\n",
        "    \"\"\"calculate similarity with SBERT all-mpnet-base-v2\"\"\"\n",
        "    model = SentenceTransformer('all-mpnet-base-v2')\n",
        "    #Encoding:\n",
        "    score = 0\n",
        "    sen = job+resume\n",
        "    sen_embeddings = model.encode(sen)\n",
        "    for i in range(len(job)):\n",
        "        if job[i] in resume:\n",
        "            score += 1\n",
        "        else:\n",
        "            if max(cosine_similarity([sen_embeddings[i]],sen_embeddings[len(job):])[0]) >= 0.4:\n",
        "                score += max(cosine_similarity([sen_embeddings[i]],sen_embeddings[len(job):])[0])\n",
        "    score = score/len(job)\n",
        "    return round(score,3)\n",
        "def match_job(job_description, resume_embeddings, filenames):\n",
        "    # Preprocess job description\n",
        "    preprocessed_job = preprocess_text(job_description)\n",
        "    # Generate job embedding\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    job_doc = nlp(preprocessed_job)\n",
        "    job_embedding = job_doc.vector\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity_scores = calculate_similarity(job_embedding, resume_embeddings)\n",
        "\n",
        "    # Rank resumes based on similarity scores\n",
        "    ranked_resumes = sorted(zip(filenames, similarity_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_resumes\n",
        "\n",
        "def main():\n",
        "    # Define input directories\n",
        "    job_description_file = \"/content/resume/job_description_embeddings.txt\"\n",
        "    resumes_directory = \"/content/resume/preprocessed_resumes\"\n",
        "\n",
        "    # Generate embeddings for resumes\n",
        "    resume_embeddings, filenames = generate_embeddings(resumes_directory)\n",
        "\n",
        "    # Load job description\n",
        "    with open(job_description_file, 'r', encoding='utf-8') as file:\n",
        "        job_desc_text = file.read()\n",
        "\n",
        "    # Match job with resumes\n",
        "    matched_resumes = match_job(job_desc_text, resume_embeddings, filenames)\n",
        "\n",
        "    # Print the top matched resumes\n",
        "    print(\"Top Matched Resumes for the Following Job Description:\\n\")\n",
        "\n",
        "    # Print job description\n",
        "    print(\"Job Description:\\n----------------\")\n",
        "    print(job_desc_text + \"\\n\")\n",
        "\n",
        "    # Print matched resumes\n",
        "    print(\"Matched Resumes:\\n----------------\")\n",
        "    print(\"| Candidate Name | Resume File  | Similarity Score (%) |\")\n",
        "    print(\"|----------------|--------------|-----------------------|\")\n",
        "    for (candidate_name, resume_file), score in matched_resumes[:5]:  # Adjust the number of top matches to display\n",
        "        similarity_percentage = score * 100\n",
        "        print(f\"| {candidate_name:<15} | {resume_file:<12} | {similarity_percentage:.2f}                   |\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52692aa4-874f-4134-9f5f-7c41bd7617e4",
        "id": "dBYK1bQiBEfo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Matched Resumes for the Following Job Description:\n",
            "\n",
            "Job Description:\n",
            "----------------\n",
            "seek DevOps engineer automate streamline organization software development deployment process candidate experience continuous integration continuous deployment CI CD tool cloud platform like aw Azure responsibility include build maintain CI cd pipeline provision infrastructure monitor system performance\n",
            "\n",
            "\n",
            "Matched Resumes:\n",
            "----------------\n",
            "| Candidate Name | Resume File  | Similarity Score (%) |\n",
            "|----------------|--------------|-----------------------|\n",
            "| 81761658        | 81761658.pdf | 95.67                   |\n",
            "| 27295996        | 27295996.pdf | 94.59                   |\n",
            "| 36434348        | 36434348.pdf | 94.49                   |\n",
            "| 26480367        | 26480367.pdf | 94.11                   |\n",
            "| 24230851        | 24230851.pdf | 94.07                   |\n"
          ]
        }
      ]
    }
  ]
}